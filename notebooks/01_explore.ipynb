{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a32ab8",
   "metadata": {},
   "source": [
    "# Mental Health Tweet Classification - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the mental health tweet dataset to understand:\n",
    "- Dataset structure and class balance\n",
    "- Text characteristics and distributions\n",
    "- Language patterns between classes\n",
    "- Visual patterns in the data\n",
    "\n",
    "**Dataset**: Depression vs Non-Depression Tweet Classification\n",
    "**Date**: September 30, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2550a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.load_data import DataLoader\n",
    "from src.data.preprocess import TweetPreprocessor, FeatureExtractor\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directories\n",
    "Path('../reports').mkdir(exist_ok=True)\n",
    "Path('../reports/figures').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45f960",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Schema Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ad4f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:src.data.load_data:Schema file not found at reports\\dataset_schema.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Schema Summary:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dataset_analysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m schema \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_schema()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Schema Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(schema[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_analysis\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m], indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dataset_analysis'"
     ]
    }
   ],
   "source": [
    "# Initialize data loader\n",
    "loader = DataLoader(dataset_dir='../dataset')\n",
    "\n",
    "# Load schema information\n",
    "schema = loader.load_schema()\n",
    "print(\"Dataset Schema Summary:\")\n",
    "print(json.dumps(schema['dataset_analysis']['summary'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c580664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files found:\n",
      "  - clean_d_tweets.csv\n",
      "  - clean_non_d_tweets.csv\n",
      "  - d_tweets.csv\n",
      "  - non_d_tweets.csv\n",
      "\n",
      "ðŸ“Š Depression tweets dataset:\n",
      "   Shape: (3496, 38)\n",
      "   Columns: ['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'tweet', 'language', 'hashtags', 'cashtags', 'user_id', 'user_id_str', 'username', 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video', 'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url', 'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src', 'trans_dest']\n",
      "\n",
      "First few rows:\n",
      "                    id      conversation_id    created_at  \\\n",
      "0  1261623711502753792  1261623711502753792  1.589630e+12   \n",
      "1  1255537912491343872  1255537912491343872  1.588179e+12   \n",
      "2  1255524270521761792  1255524270521761792  1.588175e+12   \n",
      "\n",
      "                  date  timezone  place  \\\n",
      "0  2020-05-16 01:45:07     -1000    NaN   \n",
      "1  2020-04-29 06:42:19     -1000    NaN   \n",
      "2  2020-04-29 05:48:07     -1000    NaN   \n",
      "\n",
      "                                               tweet language hashtags  \\\n",
      "0  the real reason why you're sad? you're attache...       en       []   \n",
      "1      my biggest problem is overthinking everything       en       []   \n",
      "2  the worst sadness is the sadness you've taught...       en       []   \n",
      "\n",
      "  cashtags  ...  geo  source user_rt_id user_rt  retweet_id  reply_to  \\\n",
      "0       []  ...  NaN     NaN        NaN     NaN         NaN        []   \n",
      "1       []  ...  NaN     NaN        NaN     NaN         NaN        []   \n",
      "2       []  ...  NaN     NaN        NaN     NaN         NaN        []   \n",
      "\n",
      "  retweet_date translate trans_src  trans_dest  \n",
      "0          NaN       NaN       NaN         NaN  \n",
      "1          NaN       NaN       NaN         NaN  \n",
      "2          NaN       NaN       NaN         NaN  \n",
      "\n",
      "[3 rows x 38 columns]\n",
      "\n",
      "ðŸ“Š Non-depression tweets dataset:\n",
      "   Shape: (4809, 38)\n",
      "   Columns: ['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place', 'tweet', 'language', 'hashtags', 'cashtags', 'user_id', 'user_id_str', 'username', 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video', 'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url', 'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src', 'trans_dest']\n",
      "\n",
      "First few rows:\n",
      "                    id      conversation_id    created_at  \\\n",
      "0  1424756177682927617  1424756177682927617  1.628523e+12   \n",
      "1  1424741357592084485  1424741357592084485  1.628520e+12   \n",
      "2  1424739267553550337  1424739267553550337  1.628519e+12   \n",
      "\n",
      "                  date  timezone  place  \\\n",
      "0  2021-08-09 05:35:18     -1000    NaN   \n",
      "1  2021-08-09 04:36:24     -1000    NaN   \n",
      "2  2021-08-09 04:28:06     -1000    NaN   \n",
      "\n",
      "                                               tweet language hashtags  \\\n",
      "0                      Touch Passes are the new ASMR       en       []   \n",
      "1  12 years ago, I called my dad and he gave me s...       en       []   \n",
      "2  There are 6,500 languages yet I choose to only...       en       []   \n",
      "\n",
      "  cashtags  ...  geo  source user_rt_id user_rt  retweet_id  reply_to  \\\n",
      "0       []  ...  NaN     NaN        NaN     NaN         NaN        []   \n",
      "1       []  ...  NaN     NaN        NaN     NaN         NaN        []   \n",
      "2       []  ...  NaN     NaN        NaN     NaN         NaN        []   \n",
      "\n",
      "  retweet_date translate trans_src  trans_dest  \n",
      "0          NaN       NaN       NaN         NaN  \n",
      "1          NaN       NaN       NaN         NaN  \n",
      "2          NaN       NaN       NaN         NaN  \n",
      "\n",
      "[3 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "# Quick data loading and exploration\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the dataset files directly\n",
    "dataset_dir = \"../dataset\"\n",
    "files = os.listdir(dataset_dir)\n",
    "print(\"Dataset files found:\")\n",
    "for f in files:\n",
    "    if f.endswith('.csv'):\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "# Load and examine the first file\n",
    "if 'd_tweets.csv' in files:\n",
    "    df_depression = pd.read_csv(os.path.join(dataset_dir, 'd_tweets.csv'))\n",
    "    print(f\"\\nðŸ“Š Depression tweets dataset:\")\n",
    "    print(f\"   Shape: {df_depression.shape}\")\n",
    "    print(f\"   Columns: {list(df_depression.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_depression.head(3))\n",
    "\n",
    "if 'non_d_tweets.csv' in files:\n",
    "    df_non_depression = pd.read_csv(os.path.join(dataset_dir, 'non_d_tweets.csv'))\n",
    "    print(f\"\\nðŸ“Š Non-depression tweets dataset:\")\n",
    "    print(f\"   Shape: {df_non_depression.shape}\")\n",
    "    print(f\"   Columns: {list(df_non_depression.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_non_depression.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c1365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 3,496 depression tweets\n",
      "âœ… Loaded 4,809 non-depression tweets\n",
      "\n",
      "Common columns: ['label', 'conversation_id', 'thumbnail', 'user_id_str', 'tweet', 'user_id', 'reply_to', 'translate', 'label_name', 'quote_url', 'hashtags', 'retweet_id', 'nreplies', 'photos', 'search', 'link', 'created_at', 'id', 'geo', 'user_rt', 'cashtags', 'trans_dest', 'video', 'day', 'trans_src', 'urls', 'retweet', 'language', 'nlikes', 'date', 'retweet_date', 'name', 'place', 'user_rt_id', 'username', 'near', 'hour', 'source', 'timezone', 'nretweets']\n",
      "depression: 3496 rows\n",
      "non_depression: 4809 rows\n",
      "\n",
      "ðŸ“Š Combined dataset shape: (8305, 40)\n",
      "ðŸ“Š Class distribution:\n",
      "label_name\n",
      "non_depression    4809\n",
      "depression        3496\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“‹ Sample data:\n",
      "  non_depression: Touch Passes are the new ASMR\n",
      "  depression: the real reason why you're sad? you're attached to people who have been distant with you. you're pay...\n"
     ]
    }
   ],
   "source": [
    "# Create combined dataset for analysis\n",
    "import numpy as np\n",
    "\n",
    "# Load all datasets and combine them\n",
    "datasets = {}\n",
    "\n",
    "# Load depression and non-depression tweets\n",
    "if 'd_tweets.csv' in files:\n",
    "    df_d = pd.read_csv(os.path.join(dataset_dir, 'd_tweets.csv'))\n",
    "    df_d['label'] = 1  # Depression = 1\n",
    "    df_d['label_name'] = 'depression'\n",
    "    datasets['depression'] = df_d\n",
    "    print(f\"âœ… Loaded {len(df_d):,} depression tweets\")\n",
    "\n",
    "if 'non_d_tweets.csv' in files:\n",
    "    df_nd = pd.read_csv(os.path.join(dataset_dir, 'non_d_tweets.csv'))  \n",
    "    df_nd['label'] = 0  # Non-depression = 0\n",
    "    df_nd['label_name'] = 'non_depression'\n",
    "    datasets['non_depression'] = df_nd\n",
    "    print(f\"âœ… Loaded {len(df_nd):,} non-depression tweets\")\n",
    "\n",
    "# Combine datasets\n",
    "if datasets:\n",
    "    # Find common columns\n",
    "    common_cols = set(datasets[list(datasets.keys())[0]].columns)\n",
    "    for df in datasets.values():\n",
    "        common_cols = common_cols.intersection(set(df.columns))\n",
    "    \n",
    "    print(f\"\\nCommon columns: {list(common_cols)}\")\n",
    "    \n",
    "    # Combine datasets using common columns\n",
    "    combined_dfs = []\n",
    "    for name, df in datasets.items():\n",
    "        df_subset = df[list(common_cols)].copy()\n",
    "        print(f\"{name}: {len(df_subset)} rows\")\n",
    "        combined_dfs.append(df_subset)\n",
    "    \n",
    "    # Create final combined dataset\n",
    "    df_combined = pd.concat(combined_dfs, ignore_index=True)\n",
    "    print(f\"\\nðŸ“Š Combined dataset shape: {df_combined.shape}\")\n",
    "    print(f\"ðŸ“Š Class distribution:\")\n",
    "    print(df_combined['label_name'].value_counts())\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nðŸ“‹ Sample data:\")\n",
    "    for label in [0, 1]:\n",
    "        sample = df_combined[df_combined['label'] == label].iloc[0]\n",
    "        label_name = sample['label_name']\n",
    "        # Find text column\n",
    "        text_cols = [col for col in df_combined.columns if 'tweet' in col.lower() or 'text' in col.lower()]\n",
    "        if text_cols:\n",
    "            text = str(sample[text_cols[0]])[:100] + \"...\" if len(str(sample[text_cols[0]])) > 100 else str(sample[text_cols[0]])\n",
    "            print(f\"  {label_name}: {text}\")\n",
    "else:\n",
    "    print(\"âŒ No datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8dc0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Training Baseline Model...\n",
      "Dataset: 8305 samples\n",
      "Classes: [4809 3496] (0: non-depression, 1: depression)\n",
      "Training set: 6644 samples\n",
      "Test set: 1661 samples\n",
      "\n",
      "ðŸ”§ Creating TF-IDF features...\n",
      "Feature matrix shape: (6644, 8491)\n",
      "\n",
      "ðŸŽ¯ Training Logistic Regression...\n",
      "\n",
      "ðŸ“Š Model Performance:\n",
      "Accuracy: 0.8561\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Non-Depression       0.88      0.88      0.88       962\n",
      "    Depression       0.83      0.83      0.83       699\n",
      "\n",
      "      accuracy                           0.86      1661\n",
      "     macro avg       0.85      0.85      0.85      1661\n",
      "  weighted avg       0.86      0.86      0.86      1661\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[843 119]\n",
      " [120 579]]\n"
     ]
    }
   ],
   "source": [
    "# Quick baseline model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "print(\"ðŸ¤– Training Baseline Model...\")\n",
    "\n",
    "# Prepare the data\n",
    "X = df_combined['tweet'].fillna('')  # Use tweet text as features\n",
    "y = df_combined['label']  # Use binary labels\n",
    "\n",
    "print(f\"Dataset: {len(X)} samples\")\n",
    "print(f\"Classes: {np.bincount(y)} (0: non-depression, 1: depression)\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Create TF-IDF features\n",
    "print(\"\\nðŸ”§ Creating TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Train Logistic Regression model\n",
    "print(\"\\nðŸŽ¯ Training Logistic Regression...\")\n",
    "model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred_proba = model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Depression', 'Depression']))\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving model...\n",
      "âœ… Model saved to models/baseline/\n",
      "\n",
      "ðŸ§ª Testing model on sample texts:\n",
      "\n",
      "1. Text: \"I feel so sad and hopeless, nothing matters anymor...\"\n",
      "   Prediction: Depression (confidence: 93.5%)\n",
      "\n",
      "2. Text: \"Having a great day, feeling amazing!\"\n",
      "   Prediction: Non-Depression (confidence: 59.8%)\n",
      "\n",
      "3. Text: \"Just finished a workout, feeling energized\"\n",
      "   Prediction: Depression (confidence: 72.5%)\n",
      "\n",
      "4. Text: \"Everything is falling apart, I can't take it anymo...\"\n",
      "   Prediction: Depression (confidence: 91.3%)\n",
      "\n",
      "ðŸŽ‰ Baseline model training completed successfully!\n",
      "ðŸ“‹ Summary:\n",
      "   - Model type: Logistic Regression with TF-IDF\n",
      "   - Training samples: 6,644\n",
      "   - Test accuracy: 0.8561\n",
      "   - Model saved to: models/baseline/\n"
     ]
    }
   ],
   "source": [
    "# Save the model and vectorizer\n",
    "import os\n",
    "os.makedirs('../models/baseline', exist_ok=True)\n",
    "\n",
    "print(\"ðŸ’¾ Saving model...\")\n",
    "joblib.dump(model, '../models/baseline/logistic_model.joblib')\n",
    "joblib.dump(vectorizer, '../models/baseline/tfidf_vectorizer.joblib')\n",
    "\n",
    "print(\"âœ… Model saved to models/baseline/\")\n",
    "\n",
    "# Test model on some sample predictions\n",
    "print(\"\\nðŸ§ª Testing model on sample texts:\")\n",
    "\n",
    "test_texts = [\n",
    "    \"I feel so sad and hopeless, nothing matters anymore\",\n",
    "    \"Having a great day, feeling amazing!\",\n",
    "    \"Just finished a workout, feeling energized\",\n",
    "    \"Everything is falling apart, I can't take it anymore\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    # Transform text using our vectorizer\n",
    "    text_tfidf = vectorizer.transform([text])\n",
    "    \n",
    "    # Get prediction and probability\n",
    "    pred = model.predict(text_tfidf)[0]\n",
    "    prob = model.predict_proba(text_tfidf)[0]\n",
    "    \n",
    "    label_name = \"Depression\" if pred == 1 else \"Non-Depression\"\n",
    "    confidence = max(prob) * 100\n",
    "    \n",
    "    print(f\"\\n{i}. Text: \\\"{text[:50]}{'...' if len(text) > 50 else ''}\\\"\")\n",
    "    print(f\"   Prediction: {label_name} (confidence: {confidence:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Baseline model training completed successfully!\")\n",
    "print(f\"ðŸ“‹ Summary:\")\n",
    "print(f\"   - Model type: Logistic Regression with TF-IDF\")\n",
    "print(f\"   - Training samples: {len(X_train):,}\")\n",
    "print(f\"   - Test accuracy: {accuracy:.4f}\")\n",
    "print(f\"   - Model saved to: models/baseline/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98735c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "print(\"Loading raw tweet data...\")\n",
    "texts_raw, labels_raw = loader.get_text_and_labels(use_clean=False)\n",
    "\n",
    "print(f\"Raw dataset: {len(texts_raw)} samples\")\n",
    "print(f\"Class distribution: {pd.Series(labels_raw).value_counts().to_dict()}\")\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_raw = pd.DataFrame({\n",
    "    'text': texts_raw,\n",
    "    'label': labels_raw,\n",
    "    'class_name': ['Depression' if label == 1 else 'Non-Depression' for label in labels_raw]\n",
    "})\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\")\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d985e2e",
   "metadata": {},
   "source": [
    "## 2. Class Balance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e944a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df_raw['class_name'].value_counts()\n",
    "class_props = df_raw['class_name'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "for class_name in class_counts.index:\n",
    "    count = class_counts[class_name]\n",
    "    prop = class_props[class_name]\n",
    "    print(f\"  {class_name}: {count:,} samples ({prop:.1f}%)\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_counts.plot(kind='bar', ax=ax1, color=['#ff7f7f', '#7f7fff'])\n",
    "ax1.set_title('Class Distribution (Count)')\n",
    "ax1.set_ylabel('Number of Tweets')\n",
    "ax1.set_xlabel('Class')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', \n",
    "        colors=['#ff7f7f', '#7f7fff'], startangle=90)\n",
    "ax2.set_title('Class Distribution (Proportion)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"âš ï¸  Dataset is imbalanced. Consider using class weights or resampling.\")\n",
    "else:\n",
    "    print(\"âœ… Dataset is reasonably balanced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ebae1",
   "metadata": {},
   "source": [
    "## 3. Text Length and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df_raw['text_length'] = df_raw['text'].str.len()\n",
    "df_raw['word_count'] = df_raw['text'].str.split().str.len()\n",
    "df_raw['sentence_count'] = df_raw['text'].str.count(r'[.!?]+') + 1\n",
    "\n",
    "# Summary statistics by class\n",
    "print(\"Text Length Statistics by Class:\")\n",
    "text_stats = df_raw.groupby('class_name')[['text_length', 'word_count', 'sentence_count']].describe()\n",
    "print(text_stats.round(2))\n",
    "\n",
    "# Visualize text length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Character length distribution\n",
    "for class_name in df_raw['class_name'].unique():\n",
    "    class_data = df_raw[df_raw['class_name'] == class_name]\n",
    "    axes[0, 0].hist(class_data['text_length'], alpha=0.7, label=class_name, bins=50)\n",
    "axes[0, 0].set_title('Text Length Distribution (Characters)')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "for class_name in df_raw['class_name'].unique():\n",
    "    class_data = df_raw[df_raw['class_name'] == class_name]\n",
    "    axes[0, 1].hist(class_data['word_count'], alpha=0.7, label=class_name, bins=50)\n",
    "axes[0, 1].set_title('Word Count Distribution')\n",
    "axes[0, 1].set_xlabel('Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Box plots for comparison\n",
    "sns.boxplot(data=df_raw, x='class_name', y='text_length', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Text Length by Class (Box Plot)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.boxplot(data=df_raw, x='class_name', y='word_count', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Word Count by Class (Box Plot)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/text_length_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05f0ba",
   "metadata": {},
   "source": [
    "## 4. Sample Texts by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample texts for each class\n",
    "print(\"Sample Texts by Class:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for class_name in ['Depression', 'Non-Depression']:\n",
    "    print(f\"\\n{class_name.upper()} EXAMPLES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    class_data = df_raw[df_raw['class_name'] == class_name]\n",
    "    samples = class_data.sample(n=5, random_state=42)\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        text = row['text']\n",
    "        # Truncate very long texts\n",
    "        if len(text) > 150:\n",
    "            text = text[:150] + \"...\"\n",
    "        print(f\"{i}. {text}\")\n",
    "    \n",
    "    print(f\"\\nClass statistics:\")\n",
    "    print(f\"  Total samples: {len(class_data):,}\")\n",
    "    print(f\"  Avg text length: {class_data['text_length'].mean():.1f} characters\")\n",
    "    print(f\"  Avg word count: {class_data['word_count'].mean():.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4fabe",
   "metadata": {},
   "source": [
    "## 5. Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01430bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "def get_top_words(texts, n=20, min_length=2):\n",
    "    \"\"\"Get top N words from texts.\"\"\"\n",
    "    # Combine all texts\n",
    "    all_text = ' '.join(texts).lower()\n",
    "    \n",
    "    # Extract words (remove punctuation)\n",
    "    words = re.findall(r'\\b[a-z]+\\b', all_text)\n",
    "    \n",
    "    # Filter by length and common stop words\n",
    "    stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "                  'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "                  'do', 'does', 'did', 'will', 'would', 'should', 'could', 'can', 'may',\n",
    "                  'a', 'an', 'as', 'if', 'it', 'its', 'this', 'that', 'these', 'those'}\n",
    "    \n",
    "    filtered_words = [word for word in words if len(word) >= min_length and word not in stop_words]\n",
    "    \n",
    "    return Counter(filtered_words).most_common(n)\n",
    "\n",
    "# Get top words for each class\n",
    "depression_texts = df_raw[df_raw['class_name'] == 'Depression']['text'].tolist()\n",
    "non_depression_texts = df_raw[df_raw['class_name'] == 'Non-Depression']['text'].tolist()\n",
    "\n",
    "top_depression_words = get_top_words(depression_texts, n=20)\n",
    "top_non_depression_words = get_top_words(non_depression_texts, n=20)\n",
    "\n",
    "print(\"Top 20 Words by Class:\")\n",
    "print(\"\\nDEPRESSION CLASS:\")\n",
    "for i, (word, count) in enumerate(top_depression_words, 1):\n",
    "    print(f\"{i:2d}. {word:15s} ({count:,} occurrences)\")\n",
    "\n",
    "print(\"\\nNON-DEPRESSION CLASS:\")\n",
    "for i, (word, count) in enumerate(top_non_depression_words, 1):\n",
    "    print(f\"{i:2d}. {word:15s} ({count:,} occurrences)\")\n",
    "\n",
    "# Visualize top words\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Depression words\n",
    "words_dep = [item[0] for item in top_depression_words[:15]]\n",
    "counts_dep = [item[1] for item in top_depression_words[:15]]\n",
    "ax1.barh(words_dep[::-1], counts_dep[::-1], color='#ff7f7f')\n",
    "ax1.set_title('Top 15 Words - Depression Class')\n",
    "ax1.set_xlabel('Frequency')\n",
    "\n",
    "# Non-depression words\n",
    "words_non_dep = [item[0] for item in top_non_depression_words[:15]]\n",
    "counts_non_dep = [item[1] for item in top_non_depression_words[:15]]\n",
    "ax2.barh(words_non_dep[::-1], counts_non_dep[::-1], color='#7f7fff')\n",
    "ax2.set_title('Top 15 Words - Non-Depression Class')\n",
    "ax2.set_xlabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/top_words_by_class.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585810c",
   "metadata": {},
   "source": [
    "## 6. Hashtag and Mention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852db694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags and mentions\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r'#\\w+', text.lower())\n",
    "\n",
    "def extract_mentions(text):\n",
    "    return re.findall(r'@\\w+', text.lower())\n",
    "\n",
    "# Extract for each class\n",
    "df_raw['hashtags'] = df_raw['text'].apply(extract_hashtags)\n",
    "df_raw['mentions'] = df_raw['text'].apply(extract_mentions)\n",
    "df_raw['hashtag_count'] = df_raw['hashtags'].str.len()\n",
    "df_raw['mention_count'] = df_raw['mentions'].str.len()\n",
    "\n",
    "# Statistics by class\n",
    "social_stats = df_raw.groupby('class_name')[['hashtag_count', 'mention_count']].agg(['mean', 'std', 'sum'])\n",
    "print(\"Hashtag and Mention Statistics by Class:\")\n",
    "print(social_stats.round(2))\n",
    "\n",
    "# Top hashtags by class\n",
    "print(\"\\nTop Hashtags by Class:\")\n",
    "for class_name in ['Depression', 'Non-Depression']:\n",
    "    class_data = df_raw[df_raw['class_name'] == class_name]\n",
    "    all_hashtags = [tag for tags in class_data['hashtags'] for tag in tags]\n",
    "    top_hashtags = Counter(all_hashtags).most_common(10)\n",
    "    \n",
    "    print(f\"\\n{class_name.upper()}:\")\n",
    "    for i, (hashtag, count) in enumerate(top_hashtags, 1):\n",
    "        print(f\"  {i:2d}. {hashtag:20s} ({count:,} times)\")\n",
    "\n",
    "# Visualize hashtag usage\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Hashtag count distribution\n",
    "sns.boxplot(data=df_raw, x='class_name', y='hashtag_count', ax=axes[0])\n",
    "axes[0].set_title('Hashtag Count by Class')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Mention count distribution\n",
    "sns.boxplot(data=df_raw, x='class_name', y='mention_count', ax=axes[1])\n",
    "axes[1].set_title('Mention Count by Class')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/social_features_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb54490",
   "metadata": {},
   "source": [
    "## 7. Sentiment and Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c31730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract additional features using our FeatureExtractor\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "print(\"Extracting additional features...\")\n",
    "feature_data = []\n",
    "\n",
    "# Sample a subset for performance (extract features for 1000 samples per class)\n",
    "sample_df = df_raw.groupby('class_name').apply(lambda x: x.sample(n=min(1000, len(x)), random_state=42)).reset_index(drop=True)\n",
    "\n",
    "for _, row in sample_df.iterrows():\n",
    "    features = feature_extractor.extract_all_features(row['text'])\n",
    "    features['class_name'] = row['class_name']\n",
    "    features['label'] = row['label']\n",
    "    feature_data.append(features)\n",
    "\n",
    "feature_df = pd.DataFrame(feature_data)\n",
    "\n",
    "print(f\"Extracted features for {len(feature_df)} samples\")\n",
    "print(\"\\nFeature columns:\", feature_df.columns.tolist())\n",
    "\n",
    "# Feature statistics by class\n",
    "feature_cols = ['char_count', 'word_count', 'sentence_count', 'avg_word_length', \n",
    "               'exclamation_count', 'question_count', 'uppercase_ratio',\n",
    "               'sentiment_compound', 'sentiment_positive', 'sentiment_negative']\n",
    "\n",
    "print(\"\\nFeature Statistics by Class:\")\n",
    "feature_stats = feature_df.groupby('class_name')[feature_cols].mean()\n",
    "print(feature_stats.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c690b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Features to plot\n",
    "plot_features = [\n",
    "    ('sentiment_compound', 'Sentiment (Compound Score)'),\n",
    "    ('sentiment_negative', 'Negative Sentiment'),\n",
    "    ('sentiment_positive', 'Positive Sentiment'),\n",
    "    ('exclamation_count', 'Exclamation Marks'),\n",
    "    ('question_count', 'Question Marks'),\n",
    "    ('uppercase_ratio', 'Uppercase Ratio')\n",
    "]\n",
    "\n",
    "for i, (feature, title) in enumerate(plot_features):\n",
    "    if feature in feature_df.columns:\n",
    "        sns.boxplot(data=feature_df, x='class_name', y=feature, ax=axes[i])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, f'{feature}\\nnot available', \n",
    "                    ha='center', va='center', transform=axes[i].transAxes)\n",
    "        axes[i].set_title(f'{title} (N/A)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/feature_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27409f4d",
   "metadata": {},
   "source": [
    "## 8. Text Complexity and Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d6ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text complexity\n",
    "def flesch_reading_ease(text):\n",
    "    \"\"\"Simple approximation of Flesch Reading Ease score.\"\"\"\n",
    "    sentences = len(re.findall(r'[.!?]+', text))\n",
    "    words = len(text.split())\n",
    "    syllables = sum([len(re.findall(r'[aeiouAEIOU]', word)) for word in text.split()])\n",
    "    \n",
    "    if sentences == 0 or words == 0:\n",
    "        return 0\n",
    "    \n",
    "    asl = words / sentences  # Average sentence length\n",
    "    asw = syllables / words if words > 0 else 0  # Average syllables per word\n",
    "    \n",
    "    fre = 206.835 - (1.015 * asl) - (84.6 * asw)\n",
    "    return max(0, min(100, fre))\n",
    "\n",
    "# Calculate readability for sample\n",
    "sample_df['readability'] = sample_df['text'].apply(flesch_reading_ease)\n",
    "sample_df['avg_word_length'] = sample_df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0)\n",
    "\n",
    "# Compare complexity between classes\n",
    "complexity_stats = sample_df.groupby('class_name')[['readability', 'avg_word_length']].describe()\n",
    "print(\"Text Complexity by Class:\")\n",
    "print(complexity_stats.round(2))\n",
    "\n",
    "# Visualize complexity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.boxplot(data=sample_df, x='class_name', y='readability', ax=axes[0])\n",
    "axes[0].set_title('Text Readability Score by Class')\n",
    "axes[0].set_ylabel('Flesch Reading Ease Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.boxplot(data=sample_df, x='class_name', y='avg_word_length', ax=axes[1])\n",
    "axes[1].set_title('Average Word Length by Class')\n",
    "axes[1].set_ylabel('Characters per Word')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/text_complexity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569a450",
   "metadata": {},
   "source": [
    "## 9. TF-IDF and Dimensionality Reduction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ced02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF and t-SNE visualization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"Creating TF-IDF vectors...\")\n",
    "\n",
    "# Use a smaller sample for t-SNE (computationally expensive)\n",
    "viz_sample = df_raw.groupby('class_name').apply(lambda x: x.sample(n=min(500, len(x)), random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(viz_sample['text'])\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"Example features: {feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction with PCA first (faster)\n",
    "print(\"Applying PCA for dimensionality reduction...\")\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "print(f\"Explained variance ratio (first 10 components): {pca.explained_variance_ratio_[:10].round(3)}\")\n",
    "print(f\"Total explained variance (50 components): {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Apply t-SNE\n",
    "print(\"Applying t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "tfidf_tsne = tsne.fit_transform(tfidf_pca)\n",
    "\n",
    "# Create visualization DataFrame\n",
    "viz_df = pd.DataFrame({\n",
    "    'x': tfidf_tsne[:, 0],\n",
    "    'y': tfidf_tsne[:, 1],\n",
    "    'class_name': viz_sample['class_name'].values,\n",
    "    'label': viz_sample['label'].values\n",
    "})\n",
    "\n",
    "print(\"t-SNE visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-SNE results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create scatter plot\n",
    "colors = ['#ff7f7f', '#7f7fff']\n",
    "for i, class_name in enumerate(['Depression', 'Non-Depression']):\n",
    "    class_data = viz_df[viz_df['class_name'] == class_name]\n",
    "    plt.scatter(class_data['x'], class_data['y'], \n",
    "               c=colors[i], label=class_name, alpha=0.7, s=30)\n",
    "\n",
    "plt.title('t-SNE Visualization of Tweet TF-IDF Vectors\\n(Depression vs Non-Depression)', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add sample size info\n",
    "plt.figtext(0.02, 0.02, f'Sample size: {len(viz_df)} tweets ({len(viz_df[viz_df.label==1])} Depression, {len(viz_df[viz_df.label==0])} Non-Depression)',\n",
    "           fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/tsne_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nt-SNE Interpretation:\")\n",
    "print(\"- Points close together have similar TF-IDF patterns\")\n",
    "print(\"- Clear separation suggests classes use different vocabulary\")\n",
    "print(\"- Overlap areas might be challenging for classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b04ff7",
   "metadata": {},
   "source": [
    "## 10. Most Discriminative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most discriminative TF-IDF features\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Calculate chi-squared scores for feature selection\n",
    "chi2_scores, p_values = chi2(tfidf_matrix, viz_sample['label'])\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_scores = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'chi2_score': chi2_scores,\n",
    "    'p_value': p_values\n",
    "}).sort_values('chi2_score', ascending=False)\n",
    "\n",
    "# Top discriminative features\n",
    "print(\"Top 20 Most Discriminative Features (Chi-squared test):\")\n",
    "print(\"=\" * 55)\n",
    "for i, (_, row) in enumerate(feature_scores.head(20).iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['feature']:25s} (Ï‡Â² = {row['chi2_score']:8.2f}, p = {row['p_value']:.2e})\")\n",
    "\n",
    "# Visualize top features\n",
    "top_features = feature_scores.head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(top_features)), top_features['chi2_score'], color='skyblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Chi-squared Score')\n",
    "plt.title('Top 15 Most Discriminative TF-IDF Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/discriminative_features.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de814b0a",
   "metadata": {},
   "source": [
    "## 11. Generate EDA Report Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive EDA report\n",
    "eda_report = {\n",
    "    'analysis_date': '2025-09-30',\n",
    "    'dataset_overview': {\n",
    "        'total_samples': len(df_raw),\n",
    "        'classes': df_raw['class_name'].value_counts().to_dict(),\n",
    "        'class_balance_ratio': df_raw['class_name'].value_counts().max() / df_raw['class_name'].value_counts().min(),\n",
    "        'imbalanced': (df_raw['class_name'].value_counts().max() / df_raw['class_name'].value_counts().min()) > 2\n",
    "    },\n",
    "    'text_statistics': {\n",
    "        'avg_text_length': df_raw['text_length'].mean(),\n",
    "        'avg_word_count': df_raw['word_count'].mean(),\n",
    "        'text_length_by_class': df_raw.groupby('class_name')['text_length'].mean().to_dict(),\n",
    "        'word_count_by_class': df_raw.groupby('class_name')['word_count'].mean().to_dict()\n",
    "    },\n",
    "    'vocabulary_analysis': {\n",
    "        'top_depression_words': dict(top_depression_words[:10]),\n",
    "        'top_non_depression_words': dict(top_non_depression_words[:10]),\n",
    "        'most_discriminative_features': feature_scores.head(10)[['feature', 'chi2_score']].to_dict('records')\n",
    "    },\n",
    "    'social_features': {\n",
    "        'avg_hashtags_by_class': df_raw.groupby('class_name')['hashtag_count'].mean().to_dict(),\n",
    "        'avg_mentions_by_class': df_raw.groupby('class_name')['mention_count'].mean().to_dict()\n",
    "    },\n",
    "    'complexity_analysis': {\n",
    "        'readability_by_class': sample_df.groupby('class_name')['readability'].mean().to_dict(),\n",
    "        'avg_word_length_by_class': sample_df.groupby('class_name')['avg_word_length'].mean().to_dict()\n",
    "    },\n",
    "    'key_findings': [\n",
    "        f\"Dataset contains {len(df_raw):,} tweets with {'im' if (df_raw['class_name'].value_counts().max() / df_raw['class_name'].value_counts().min()) > 2 else ''}balanced classes\",\n",
    "        f\"Depression tweets average {df_raw[df_raw.class_name=='Depression']['text_length'].mean():.1f} characters vs {df_raw[df_raw.class_name=='Non-Depression']['text_length'].mean():.1f} for non-depression\",\n",
    "        f\"Most discriminative feature: '{feature_scores.iloc[0]['feature']}' (Ï‡Â² = {feature_scores.iloc[0]['chi2_score']:.1f})\",\n",
    "        \"t-SNE visualization shows distinguishable clustering patterns between classes\"\n",
    "    ],\n",
    "    'recommendations': [\n",
    "        \"Use TF-IDF features as they show clear discriminative patterns\",\n",
    "        \"Consider class weighting due to imbalance\" if (df_raw['class_name'].value_counts().max() / df_raw['class_name'].value_counts().min()) > 2 else \"Classes are reasonably balanced\",\n",
    "        \"Include sentiment and social features (hashtags, mentions) in model\",\n",
    "        \"Text preprocessing should preserve emotional indicators while removing noise\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save report as JSON\n",
    "with open('../reports/eda_report.json', 'w') as f:\n",
    "    json.dump(eda_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"EDA Report Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for finding in eda_report['key_findings']:\n",
    "    print(f\"â€¢ {finding}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"-\" * 30)\n",
    "for rec in eda_report['recommendations']:\n",
    "    print(f\"â€¢ {rec}\")\n",
    "\n",
    "print(f\"\\nâœ… Complete EDA report saved to: reports/eda_report.json\")\n",
    "print(f\"âœ… All visualizations saved to: reports/figures/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HTML report\n",
    "html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Mental Health Tweet Classification - EDA Report</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}\n",
    "        .header {{ background-color: #f4f4f4; padding: 20px; border-radius: 10px; }}\n",
    "        .section {{ margin: 30px 0; }}\n",
    "        .metric {{ background-color: #e8f4f8; padding: 15px; margin: 10px 0; border-radius: 5px; }}\n",
    "        .finding {{ background-color: #f0f8e8; padding: 10px; margin: 5px 0; border-radius: 3px; }}\n",
    "        .recommendation {{ background-color: #fff8e0; padding: 10px; margin: 5px 0; border-radius: 3px; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; margin: 10px 0; }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "        th {{ background-color: #f2f2f2; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>Mental Health Tweet Classification</h1>\n",
    "        <h2>Exploratory Data Analysis Report</h2>\n",
    "        <p><strong>Date:</strong> {eda_report['analysis_date']}</p>\n",
    "        <p><strong>Dataset:</strong> Depression vs Non-Depression Tweet Classification</p>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h3>Dataset Overview</h3>\n",
    "        <div class=\"metric\">\n",
    "            <strong>Total Samples:</strong> {eda_report['dataset_overview']['total_samples']:,}<br>\n",
    "            <strong>Classes:</strong> {eda_report['dataset_overview']['classes']}<br>\n",
    "            <strong>Class Balance Ratio:</strong> {eda_report['dataset_overview']['class_balance_ratio']:.2f}<br>\n",
    "            <strong>Imbalanced:</strong> {eda_report['dataset_overview']['imbalanced']}\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h3>Key Findings</h3>\n",
    "        {''.join([f'<div class=\"finding\">â€¢ {finding}</div>' for finding in eda_report['key_findings']])}\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h3>Recommendations</h3>\n",
    "        {''.join([f'<div class=\"recommendation\">â€¢ {rec}</div>' for rec in eda_report['recommendations']])}\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h3>Text Statistics</h3>\n",
    "        <table>\n",
    "            <tr><th>Metric</th><th>Depression</th><th>Non-Depression</th></tr>\n",
    "            <tr><td>Avg Text Length</td><td>{eda_report['text_statistics']['text_length_by_class']['Depression']:.1f}</td><td>{eda_report['text_statistics']['text_length_by_class']['Non-Depression']:.1f}</td></tr>\n",
    "            <tr><td>Avg Word Count</td><td>{eda_report['text_statistics']['word_count_by_class']['Depression']:.1f}</td><td>{eda_report['text_statistics']['word_count_by_class']['Non-Depression']:.1f}</td></tr>\n",
    "        </table>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"section\">\n",
    "        <h3>Generated Visualizations</h3>\n",
    "        <ul>\n",
    "            <li>Class Distribution (figures/class_distribution.png)</li>\n",
    "            <li>Text Length Analysis (figures/text_length_analysis.png)</li>\n",
    "            <li>Top Words by Class (figures/top_words_by_class.png)</li>\n",
    "            <li>Social Features Analysis (figures/social_features_analysis.png)</li>\n",
    "            <li>Feature Analysis (figures/feature_analysis.png)</li>\n",
    "            <li>Text Complexity (figures/text_complexity.png)</li>\n",
    "            <li>t-SNE Visualization (figures/tsne_visualization.png)</li>\n",
    "            <li>Discriminative Features (figures/discriminative_features.png)</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Save HTML report\n",
    "with open('../reports/eda_report.html', 'w') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(\"âœ… HTML report generated: reports/eda_report.html\")\n",
    "print(\"\\nðŸŽ‰ Exploratory Data Analysis Complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the generated visualizations and reports\")\n",
    "print(\"2. Proceed with feature engineering and model building\")\n",
    "print(\"3. Use insights from EDA to guide preprocessing and model selection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
